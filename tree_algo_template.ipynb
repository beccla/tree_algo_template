{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model for [Target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "frm src import utils, write, version_name\n",
    "from src.modelling import prep, train, visualisations as viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XBGRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.read_config('gcs_paths.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'wtr_whitespace': False,\n",
    "    'jl_whitespace': False,\n",
    "    'print_viz': 'nb'\n",
    "}\n",
    "write.dict_to_yaml(model_config, 'model_config.yaml', base=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.read_config('target_variables.yaml')  # list all possible target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.dict_to_yaml(\n",
    "    {'targets': ['jl_branch_sales']},\n",
    "    'target_variables.yaml',\n",
    "    base=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Training Data, Features and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = utils.read_config('target_variables.yaml')['targets'][0]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_at = prep.import_analytics_table()\n",
    "features = prep.get_features()\n",
    "n_features = len(features)\n",
    "n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params = {\n",
    "    'max_depth': -1  # growing an unpruned tree for feature importances\n",
    "    'colsample+bytree': 0.2,  # consider only a smaller number of features per tree\n",
    "    'random_state': 171,\n",
    "    'n_jobs': -1\n",
    "    'objective': 'mse'\n",
    "    }\n",
    "\n",
    "algo_base = LGBMRegressor(**tree_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce dataframe of feature importances and plot results.\n",
    "fi_df = train.fit_n_iterations(\n",
    "    n=100,\n",
    "    algo=algo_base,\n",
    "    df=df_at,\n",
    "    regressors=features,\n",
    "    target=target,\n",
    "    feat_imp=True,\n",
    "    quantile_bool=False\n",
    ")\n",
    "\n",
    "fi_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_top = fi_df[fi_df['imp'] > fi_df['imp'].describe()['75%']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination  \n",
    "Included this step, as initially ran a feature importance and retained an arbitary 200 features.  We removed correlated variables and now running RFE will tell us if there's no improvement after the nth variable.  RFE won't improve the model perfomance but it will simplify the interpretability of the model and maintenance cost associated with gathering and maintaining the code base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfe_res will return a variable that avoids elimination in at least 1 of the n iterations.\n",
    "# below we choose how many iterations (n) a variable needs to survive - giving more confidence that it's a \n",
    "# predictive variable\n",
    "n = 10\n",
    "rfe_res = train.rfe_n_iterations(n, algo_base, df_at, fi_top.cols, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appearances = 3\n",
    "print(f\"\"\"\n",
    "Shortlisted features: {len(fi_top.cols)}\n",
    "Features making at least one iteration of RFE: {len(ref_res)}\n",
    "Features making {appearances} of {n} appearances in the RFE iterations: {len(rfe_res[rfe_res > appearances])}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_shortlist = (rfe_res[rfe_res > (n / 2)]).index\n",
    "np.sort(ref_shortlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_shortlist = np.unique(ref_shortlist.tolist() + ['drivetime'])\n",
    "len(final_shortlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn, X_val, y_trn, y_val = (train._create_postcode_area_train_test_split(\n",
    "    df_at,\n",
    "    final_shortlist,\n",
    "    target,\n",
    "    171\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Partial Dependence Plots  \n",
    "PDP recap;  \n",
    "* Overwrite the selected feature so that each observation has the same value and predict the target variable.  Repeat this for each value observed for that feature.  \n",
    "* Eg/ overwrite the drivetime to 10 for all postcodes, make a prediction.  Then overwrite the drivetimes to 11 for all postcodes, make a prediction... 12, 13, ... n.\n",
    "* Now we can isolate the effect of this one variable, all other variables being equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = viz.feature_importances_df(X_trn, algo_base)[-10:]\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_partial_dependence(\n",
    "    algo_base,\n",
    "    X_trn,\n",
    "    list(top_10['feature']),\n",
    "    figsize=(12, 8)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Values  \n",
    "* Variables are ranked by feature importance  \n",
    "* Each dot is an observation  \n",
    "* The horizontal axis shows whether the value of the observation has a positive or negative effect on the prediction.  \n",
    "* The colour shows whether the variables value is high (red) or low (blue)  \n",
    "* observations centred around zero confirm that the variable is adding little value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.shap_vis(\n",
    "    X_trn, algo_base,\n",
    "    interaction_plot=False,\n",
    "    dependence_plot=False,\n",
    "    max_display=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn, X_val, y_trn, y_val = (train._create_postcode_area_train_test_split(\n",
    "    df_at,\n",
    "    final_shortlist,\n",
    "    target,\n",
    "    171\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy = LazyRegressor(predictions=True)\n",
    "\n",
    "model_res, _ = lazy.fit(X_trn, X_val, y_trn, y_val)\n",
    "model_res[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch Parameters  \n",
    "This approach looks at hyper-parameters in logical pairs.  i.e assess the hyper-parameters controlling tree depth such as max depth and minium  number of observations per leaf node.  \n",
    "\n",
    "* `max_depth`: maximum depth of each tree.  \n",
    "* `min_child_weight` (xgboost) or `min_samples_leaf` (sklearn): minimum number of observations per leaf node.  \n",
    "* `n_estimators`: number of decision trees.  \n",
    "* `subsample`: proportion of samples used per tree.  \n",
    "* `colsample_bytree`: proportion of features used per tree.  \n",
    "\n",
    "Regularisation Parameters  \n",
    "* `learning_rate`: values less than 1 make fewer corrections in the sequential gradient boosting and protect from overfitting.  \n",
    "* `gamma`: min loss reduction required to make a split, higher value = fewer splits.  \n",
    "* `alpha`: L1 regularisation - higher val = more regularisation  \n",
    "* `lambda_`: L2 regularisation - smoother than L1... what does this mean?!  \n",
    "\n",
    "The scoring metric is the negative median absolute error to follow the convention that higher values are better, thus the best_param attribute is still relevant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params = {\n",
    "    'random_state': 171,\n",
    "    'n_jobs': -1,\n",
    "    'objective': 'mse',\n",
    "    'n_estimators': 2000\n",
    "    }\n",
    "\n",
    "winning_algo = LGBMRegressor(**tree_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training, val and test sets  \n",
    "We will use the validation set to tune the hyper-parameters, so it's necessary to create an additional test set of unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn, X_val, X_test, y_trn, y_val, y_test = (train._create_postcode_area_train_test_split(\n",
    "    df_at,\n",
    "    final_shortlist,\n",
    "    target,\n",
    "    171,\n",
    "    test_size=0.15,\n",
    "    test_set=True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Base prediction (without tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with eval set\n",
    "eval_set = [(X_trn, y_trn), (X_val, y_val)]\n",
    "\n",
    "winning_algo.fit(\n",
    "    X_trn,\n",
    "    y_trn,\n",
    "    eval_metric='mse',\n",
    "    eval_set=eval_set,\n",
    "    early_stopping_rounds=10,  # if lightgbm include this line.\n",
    "    # if using early stopping set verbose to 50, else False.\n",
    "    verbose=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_params['n_estimators'] = 175 # set this based on the verbose above.\n",
    "winning_algo = LGBMRegressor(**tree_params)\n",
    "base_model = winning_algo.fit(X_trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check scores across training, val and test sets for overfitting.\n",
    "train.train_val_test_scores(\n",
    "    base_model,\n",
    "    '_rmse',\n",
    "    df_at,\n",
    "    final_shortlist,\n",
    "    target,\n",
    "    171,\n",
    "    test_set=True,\n",
    "    test_sise=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 10\n",
    "train.fit_n_iterations(\n",
    "    n_iter,\n",
    "    base_model,\n",
    "    df_at,\n",
    "    final_shortlist,\n",
    "    target,\n",
    "    feat_imp=False,\n",
    "    quantile_bool=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns...  \n",
    "Avg score over 10 samples (one standard deviation in brackets)  \n",
    "R2: 88 (87 - 89%)  \n",
    "MAE: £70k (£65-72k)  \n",
    "Mean APE: 139% (100 - 175%)  \n",
    "Median APE: 28% (27 - 30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset\n",
    "tree_params['n_estimators']  = 100\n",
    "winning_algo = LGBMRegressor(**tree_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters controlling the architecture/ depth of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [-1]\n",
    "min_child_weight = [int(x) for x in np.linspace(start=20, stop=300, num=5)]  # may need to widen this initially to search the right space.\n",
    "\n",
    "param_grid - {\n",
    "    'max_depth': max_depth,\n",
    "    'min_child_weight': min_child_weight  # min_samples_leaf if sklearn algo.\n",
    "}\n",
    "\n",
    "print(train.check_gridsearch_iter(param_grid))  # checks the number of parameters searched to warn if enormous!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gridsearch.\n",
    "gs_results, best_params, best_algo1 = train.fit_gridsearch_plot_results(\n",
    "    randomgrid=False,\n",
    "    estimator=winning_algo,\n",
    "    scoring_metric='neg_mean_squared_error',\n",
    "    param_grid=param_grid,\n",
    "    X=X_trn,\n",
    "    y=y_trn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^^ Returns...  \n",
    "Best score: xxxx  \n",
    "Best params: {'max_depth': x, 'min_child_weight': y}\n",
    "\n",
    "Then a plot of each parameter vs corresponding effect on neg_mean_squared_error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check scores across training, val and test sets for overfitting.\n",
    "train.train_val_test_scores(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramters controlling the sampling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = [1 / 10.0 for i in range(6, 11)]\n",
    "colsample_bytree = [1 / 10.0 for i in range(6, 10)]\n",
    "subsample_freq = [2, 4, 8, 12, 14]\n",
    "\n",
    "param_grid = {\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'subsample_freq': subsample_freq\n",
    "}\n",
    "print(train.check_gridsearch_iter(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gridsearch.\n",
    "gs_results, best_params, best_algo2 = train.fit_gridsearch_plot_results(\n",
    "    randomgrid=False,\n",
    "    estimator=winning_algo,\n",
    "    scoring_metric='neg_mean_squared_error',\n",
    "    param_grid=param_grid,\n",
    "    X=X_trn,\n",
    "    y=y_trn\n",
    ")\n",
    "\n",
    "# check scores across training, val and test sets for overfitting.\n",
    "train.train_val_test_scores(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = [x.round(2) for x in np.linspace(start=0.01, stop=0.2, num=5)]\n",
    "param_grid = {\n",
    "    'learning_rate': learning_rate,\n",
    "    'n_estimators': [2000]  # increase number of trees to see effect of lower learning rate/ higher trees combo.\n",
    "}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gridsearch.\n",
    "gs_results, best_params, best_algo3 = train.fit_gridsearch_plot_results(\n",
    "    randomgrid=False,\n",
    "    estimator=winning_algo,\n",
    "    scoring_metric='neg_mean_squared_error',\n",
    "    param_grid=param_grid,\n",
    "    X=X_trn,\n",
    "    y=y_trn\n",
    ")\n",
    "\n",
    "# check scores across training, val and test sets for overfitting.\n",
    "train.train_val_test_scores(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Trees  \n",
    "\n",
    "nb use a gridsearch here if its Random Forest, for XGBoost because it's sequentially learning we plot the improvement across the training and validation set by passing the optional `eval_set`.  To start set a high number of trees and analyse the plot for the point of inflection; the point with the validation score plateaus/ worsens whilst the training score continues to improve/ overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = [(X_trn, y_trn), (X_val, y_val)]\n",
    "\n",
    "best_algo3.fit(\n",
    "    X_trn,\n",
    "    y_trn,\n",
    "    eval_set=eval_set,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "train._rmse(y_test, best_algo3.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think this code could be used with XGBoost...\n",
    "# which would plot the training & validation scores over n_estsimators;\n",
    "\n",
    "# train.plot_early_stopping(best_algo3, xlim=2000, ylim=35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the number of trees from the verbose;\n",
    "best_params = best_algo3.get_params()\n",
    "best_params['n_estimators'] = 1723 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit confidence interval models...  \n",
    "Here the model's objective is switched to `quantile` so that the algorithm trains to minimise the error around the given quantiles instead of the mean error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_dict = {}\n",
    "# save MDE model as the 50th percentile (this was a cheat that meant we didn't need to change the GCS folder structure.)\n",
    "\n",
    "model = LGBMRegressor(**best_params).fit(X_trn, y_trn)\n",
    "mod_dict[50] = model\n",
    "\n",
    "# switch the objective\n",
    "best_params['objective'] = 'quantile'\n",
    "quantiles = ['.05', '.3', '.7', '.95']\n",
    "\n",
    "for q in qunatiles:\n",
    "    model = LGBMRegressor(alpha=q, **best_params).fit(X_trn, y_trn)\n",
    "    mod_dict[np.int(q * 100)] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Evaluation Metrics  \n",
    "Fit on multiple test/train splits to check confidence in the model.  \n",
    "* $R^2$ : The proportion of the variance for a dependent variables that's explained by independent variables.  \n",
    "* Mean Absolute Error: Average magnitude of errors without considering direction and each observation has the same weight.  \n",
    "* Root Mean Squared Error: Also measures the average magnitude of the error, commonly described as the standard deviation of the residuals.  By squaring before taking the mean results in larger errors being penalised more, this RMSE is always equal to or larger than MAE.  This can be helpful if being £10k out is more than twice as bad as being £5k out.  But if it;s just twice as bad then MAE is more appropriate.  \n",
    "* Mean Absolute Percentage Error: can only be calculated on observations with existing sales/ target variable to avoid \"devide by zero\" error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check scores across train, val and test sets...\n",
    "train.train_val_test_scores(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this plot showed a scatter plot of the observations true value vs predicted.  \n",
    "# therefore if the prediction was completely accurate then you'd have a diagnal line plotted, deviation from this shows the error.\n",
    "viz.plot_regression(\n",
    "    mod_dict[50],\n",
    "    mod_dict[5],\n",
    "    mod_dict[95],\n",
    "    df_at,\n",
    "    final_shortlist,\n",
    "    target,\n",
    "    test_set=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle model and save to GCS\n",
    "\n",
    "**Set model id** \n",
    "Using the naming convention {target}_{training_data_year}_{scenario}, eg/ \"wtr_online_sales_TY19_base\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs = utils.read_config('gcs_paths.yaml')\n",
    "from src import version_name\n",
    "model_id = f\"{target}_FY19_base\"\n",
    "model_path = f\"{version_name}_/{gcs['model_output_path']}/{model_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "for mod in mod_dict:\n",
    "    file_name = f\"{target}_FY19_{mod}_base.pickle\"\n",
    "    write.pickle_to_gcs(mod_dict[mod], model_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save meta data\n",
    "write.save_model_details(X_trn, mod_dict[50], model_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
